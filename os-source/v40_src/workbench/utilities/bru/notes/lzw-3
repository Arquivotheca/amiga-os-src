Article 375 of comp.sys.amiga.tech:
Path: fishpond!mcdphx!asuvax!cs.utexas.edu!usc!snorkelwacker!mit-eddie!uw-beaver!sumax!polari!tm
From: tm@polari.UUCP (Toshi Morita)
Newsgroups: comp.sys.amiga.tech
Subject: compression
Keywords: LZ Huffman compression
Message-ID: <2008@polari.UUCP>
Date: 14 May 90 05:15:46 GMT
Distribution: na
Organization: Seattle Online Public Unix (206) 328-4944
Lines: 240



fbueller@bucsf.bu.edu (Peter Sherman) writes:

>        Hello all. I was just wondering if anyone would care to explain to
>me how the compression algorithms in arc, zoo, zip and lharc work. I've
>heard the terms "Huffman" and "Lempel-Ziv" all over the place, but I still
>don't understand how they are able to squeeze data. Since I would like to
>develop my own utility to squeeze programs (just for the hell of it, not
>to compete with the ones already in existence) any information on any
>aspect of data compression no matter how simple or abstact will be gladly
>accepted via e-mail or via usenet (so everyone can learn too).

>        Thanks a great deal (in advance).

Huffman encoding:

  Summary:

        A compression scheme which compresses data by analyzing the frequency
of occurence of bytes and builds a uniquely decodable variable-length bit
pattern for each byte and outputs the data using the new bit pattern.
Compression is achieved by assigning shorter-length bit patterns to the most
frequently occuring bytes.

  Example:

For demonstration restrict the input permutations to eight, i.e. handle the
input stream as a sequence of three bits.

  Huffman step 1:

Do a preliminary pass through the data collecting frequency of occurence for
each permutation.

0:  25 counts
1:  25 counts
2:  25 counts
3: 100 counts
4: 110 counts
5: 130 counts
6: 150 counts
7: 160 counts

  Huffman step 2:

Build table of variable length bit-codes. Build table by merging two most
infrequently occuring permutations until unity is reached. The algorithm builds
a tree in reverse.

Initially, in this example, 0 and 1 are most infreuently occuring. Merge their
nodes and assign the resulting node the sum of their frequency counts.

0:  25 counts --__ 50 counts
1:  25 counts --
2:  25 counts
3:  75 counts
4: 110 counts
5: 130 counts
6: 150 counts
7: 160 counts

Repeat process until only no more nodes remain to be merged.


0:  25 counts --___
1:  25 counts --   \
2:  25 counts -------- 75 counts
3:  75 counts
4: 110 counts
5: 130 counts
6: 150 counts
7: 160 counts

0:  25 counts --__ 
1:  25 counts --  \
2:  25 counts ------\
3:  75 counts --------- 150 counts
4: 110 counts
5: 130 counts
6: 150 counts
7: 160 counts

0:  25 counts --__ 
1:  25 counts --  \
2:  25 counts ------\
3:  75 counts --------- 150 counts
4: 110 counts --__ 240 counts
5: 130 counts --
6: 150 counts
7: 160 counts

0:  25 counts --__ 
1:  25 counts --  \
2:  25 counts ------\
3:  75 counts --------- 150 counts
4: 110 counts --__ 240 counts
5: 130 counts --
6: 150 counts --__ 310 counts
7: 160 counts --

0:  25 counts --__ 
1:  25 counts --  \
2:  25 counts ------\
3:  75 counts ---------\
4: 110 counts --________\__ 390 counts
5: 130 counts --
6: 150 counts --__ 310 counts
7: 160 counts --

0:  25 counts --__ 
1:  25 counts --  \
2:  25 counts ------\
3:  75 counts ---------\
4: 110 counts --________\___
5: 130 counts --            \
6: 150 counts --_____________\ 700 counts
7: 160 counts --

Next, assign each decision branch a 0 or 1 bit. For ease of visual
interpretation ssign a "0" to each lower branch and a "1" to each high branch.
For example, the bit pattern "100" (4) would be encoded as "101".

  Huffman step 3:

Build translation table.

000 (0) -> "11111"
001 (1) -> "11110"
010 (2) -> "1110"
011 (3) -> "110"
100 (4) -> "101"
101 (5) -> "100"
110 (6) -> "01"
111 (7) -> "00"

  Huffman step 4:

Make another pass through input data. For each input data do index into
translation table and output variable length equivalent.

In this case, input data is 700 pieces of 3 bits (2100 bits). Output data will
be:

0: 5 bits x  25 occurences:  125 bits
1: 5 bits x  25 occurences:  125 bits
2: 4 bits x  25 occurences:  100 bits
3: 3 bits x  75 occurences:   75 bits
4: 3 bits x 110 occurences:  330 bits
5: 3 bits x 130 occurences:  390 bits
6: 2 bits x 150 occurences:  300 bits
7: 2 bits x 160 occurences:  320 bits
----------------------------------------
                            1765 bits output data

Data was compressed from 2100 bits to 1765 bits for a savings of 335 bits.

The exact same process is used when processing input as a collection of 8 bit
bytes; for brevity only 3 bits were used.


Lempel-Ziv:

Summary:

Lempel-Ziv is the algorithm originally described by Abram Lempel and Jacob Ziv
in the "IEEE Transactions on Information Theory". The basic premise of
Lempel-Ziv is to encode the current input in terms of the past input. There
seem to be quite a few implementations of Lempel-Ziv including one which
maintains the past input as a circular buffer (which will be subsequently 
referenced as simple LZ (SLZ) for lack of a better name), one using a linked
list also accessed as an array (Lempel-Ziv-Welch) and one using an array with
open hashing (Shrinkit).

The easiest implementation to explain is SLZ. SLZ maintains a circular history
buffer of the previous input and parses off the longest string from the current
input that matches an identical string in the history buffer and encodes said
string as a length and index of the matching buffer location. If no bytes in
buffer match initial byte of string then single byte is encoded as "00XX" where
00 is the length (designating following verbatim byte) and XX is the verbatim
byte. The buffer is then updated to reflect the current past n bytes.

  SLZ Step 1:

Allocate a buffer of 2^n bytes. Preload buffer with 2^n bytes of verbatim data.
Copy same 2^n bytes to output. Fetch an input byte for next step.

  SLZ Step 2:

Attempt to find bytes in buffer equal to current input byte. Store indexes of
identical bytes in separate index array. Set length counter to one.

  SLZ Step 3:

If no matches then:

    Output "00"
    Output verbatim unfound byte
    Update history buffer
    Fetch new current input byte
    Loop to step 2.

  SLZ Step 4:

Fetch an input byte. Attempt to match new input byte with byte at (buffer index
fetched from previously built index array plus length counter offset). If no
match then delete index entry from index array. Repeat until index array is
cleared.

 SLZ Step 5:

Output length counter and most recently deleted index array entry. (Resurrect
longest matching parsed string matching input.) Update history buffer. Preserve
current input byte and Loop to Step 2.


The other implementations of Lempel-Ziv are fairly similar differing only in
the implementation of the "history" buffer and speed of compression/
decompression. SLZ is very slow on compression and blindingly fast on
decompression; LZW is moderately fast on compression and decompression and
supports variable length bit codes (an additional 10% or so in compression);
Open-array hashing is slightly faster than LZW on both compression and
decompression but only supports fixed length bit codes.

There are a few other compression schemes that I'm aware of that haven't been
covered here: the adaptive variant of Huffman encoding, run-length encoding
(RLE, aka Macintosh "PackBytes") and a new scheme that I've taken the liberty
of naming "intercharacter dependency coding" (ICDC).

BTW if any OEMs are searching for a data-compression consultant/specialist
(perhaps for archival backup systems, high-volume data transmission systems,
etc) I'm interested. Send email, please.

If any part of this message is reproduced/copied/quoted please include my name.



Toshiyasu Morita / Software Engineer @ Taito Software, Inc.
Any opinions in this message are strictly my own!
tm@polari.UUCP


